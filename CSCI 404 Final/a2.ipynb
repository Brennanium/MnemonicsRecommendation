{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of a2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "I0Rj7pS1Ix3z",
        "E14MB9ZWagXa",
        "YnRBwKOz0n0t",
        "foG7qAwPsW7M",
        "xnm5kzNx7oID",
        "2JYmPlq_d1Xl",
        "CKPfP_WFxG76"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu8lNaAGd5Rl"
      },
      "source": [
        "# **Assignment 2: N-gram Language Models**\n",
        "\n",
        "Due: 5/6 (Thursday) 11:59pm on Canvas. Please work in a group of 2 on this assignment. \n",
        "\n",
        "In this assignment, which is based on [Chapter 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf) of Jurasky and Martin's [book](https://web.stanford.edu/~jurafsky/slp3/), you will get practicallly acquinted with n-gram language models, one of the simplest and yet most useful tools in the language technologiest toolbox. You will extract n-gram statistics from text and to use these statistics for language modeling and generation. \n",
        "\n",
        "**The learning goals of this assignment are**:\n",
        "- Experiment with language models and implement basic smoothing.\n",
        "- Have fun using language models to probabilistically generate sentences\n",
        "- Understand how to evaluate your language models by computing the perplexity. \n",
        "\n",
        "**Writing Code**\n",
        "\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space. Include the output in your code. \n",
        "\n",
        "\n",
        "**Submission**\n",
        "- link to your code file (**include output**. same as previoius assignments)\n",
        "\n",
        "The submission link will be closed at 11:59pm 5/6 (Thursday). No late submission will be accepted.\n",
        "\n",
        "**This exercise will take 12% of the total final grade.** If you wish to discuss any of it, feel free to bring it to the instructor during office hours. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0M0DhEsix5C"
      },
      "source": [
        "**Attention:** DO NOT edit this file. All your changes to this file cannot be saved and will get lost. Make your own copy of this file for editing.\n",
        "\n",
        "See the instruction from the previoius assignment for making a copy of this file. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oet-QbrDkLEY"
      },
      "source": [
        "0-1. Mount Google Drive\n",
        "\n",
        "You need to mount Google drive first on Google Colab space so you can access the files in your Google drive. When you do so, you will be asked to go to an URL to retrieve an authorization code and enter it here.\n",
        "\n",
        "Goal in thie following cell: load the Drive helper and mount\n",
        "\n",
        "It will show Drive mounted at /content/gdrive after mounted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnnV-v3SkSVH",
        "outputId": "c5b37a6f-df90-434b-f5bd-bf5978754580"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "# explaination of drive.mount()\n",
        "# This will prompt for authorization for accessing the data on your Google Drive\n",
        "# You need to (i) click the link showed after execute, (ii) copy the code, \n",
        "# (iii) paste below and (iv) press \"Enter\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCU-rsQGgX9k"
      },
      "source": [
        "# **Training data and test data**\n",
        "\n",
        "For this assignment, you are provided a [training data set](https://drive.google.com/file/d/1_kmHPZO6Me3arN_XtDg8CKWQvDxUCuf5/view?usp=sharing) and a [test data set](https://drive.google.com/file/d/1oaA1q1yJ9YD025-sqRm-5UCo5crYqnuk/view?usp=sharing).\n",
        "You will notice they have one sentence per line, and have been tokenized with punctuations removed.\n",
        "You need to surround each sentence by a start of sentence and end of sentence marker (`<s>` and `</s>`).\n",
        "No need to further process the corpus unless you are asked to. Use the training data to build N-gram models. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0Rj7pS1Ix3z"
      },
      "source": [
        "# **Task 0 - Process the training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjKu8MUA0D7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3f5ab4-f037-472d-de5f-07ba7f2ab75a"
      },
      "source": [
        "#TODO: import the needed libraries, \n",
        "#padding each sentence with <s> at the beginning and </s> at the end\n",
        "#you may want to put the padded training data into a list for the convenience of subsequent steps\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import heapq\n",
        "import operator\n",
        "import math\n",
        "\n",
        "\n",
        "## Loads in the words and labels of one of the datasets\n",
        "def load_file(data_file):\n",
        "    sentences = []   \n",
        "    with open(data_file, 'rt', encoding=\"utf8\") as f:\n",
        "        i = 0\n",
        "        for line in f:\n",
        "            sentences.append(\"<s> \" + line + \" </s>\")\n",
        "    return sentences\n",
        "\n",
        "# file paths\n",
        "cwd = \"/content/gdrive/MyDrive/1 - School/1 - SP21/CSCI 404/Assignment 2/\"\n",
        "\n",
        "training_file = cwd + \"data/train.txt\"\n",
        "test_file = cwd + \"data/test.txt\"\n",
        "\n",
        "\n",
        "sentences = load_file(training_file)\n",
        "\n",
        "\n",
        "\n",
        "# See References section at the end of this document\n",
        "def train_ngram(ngrams):\n",
        "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "    for ng in ngrams:\n",
        "        model[ng[:-1]][ng[-1]] += 1  # [(w1, w2, ... , wn-1)][wn]\n",
        "    \n",
        "    total_count = 0      \n",
        "    for hist in model: # hist = (w1, w2, ... , wn-1)\n",
        "        total_count = float(sum(model[hist].values()))\n",
        "        for w in model[hist]: # w = wn\n",
        "            model[hist][w] /= total_count\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E14MB9ZWagXa"
      },
      "source": [
        "#**Task 1 - Generate n-grams from text** \n",
        "You may find the online resource [1](https://www.programcreek.com/python/example/86315/nltk.bigrams) helpful for using NLTK to extract n-grams and their statistics. \n",
        "\n",
        "**1.1 Extract unigrams from training data**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_Kfjl4yzlAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70cdd1f-90da-4665-a7e6-8d98be2e9a80"
      },
      "source": [
        "#TODO: extract unigrams from training data,\n",
        "# output top 50 most frequent unigrams (sounds familiar? yes, we did something similar in assignment 0) and their probabilities \n",
        "\n",
        "unigrams = [ug for sent in sentences for ug in sent.split()]\n",
        "\n",
        "unigram_fdist = nltk.FreqDist(unigrams)\n",
        "\n",
        "unigram_count = len(unigrams)\n",
        "for ug, freq in unigram_fdist.most_common(50): \n",
        "    print(f\"{str(ug):<15} {freq/unigram_count}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the             0.046396693775689356\n",
            "<s>             0.04087814429576155\n",
            "</s>            0.04087814429576155\n",
            "of              0.02472991469412588\n",
            "to              0.02379107998013322\n",
            "in              0.018507579829906043\n",
            "and             0.017952999672293544\n",
            "said            0.01682953200656503\n",
            "a               0.016813862051251655\n",
            "mln             0.011665941079605417\n",
            "for             0.009064047195180194\n",
            "dlrs            0.008120443364353032\n",
            "vs              0.007922184364518588\n",
            "it              0.007125060550751238\n",
            "pct             0.006801441908409793\n",
            "on              0.006238686121938142\n",
            "is              0.0056425465176249525\n",
            "from            0.005237852889096914\n",
            "its             0.00499667183775192\n",
            "that            0.004994627930537132\n",
            "at              0.00493262941168856\n",
            "by              0.004845422703857603\n",
            "be              0.004775248556149878\n",
            "cts             0.004544287040878826\n",
            "year            0.004524529271135874\n",
            "will            0.004520441456706298\n",
            "with            0.0043569288795232515\n",
            "billion         0.004001970326555056\n",
            "net             0.003896368453791005\n",
            "was             0.003773052718498791\n",
            "us              0.0035080260829812703\n",
            "he              0.003287284103784158\n",
            "has             0.0032709328460658535\n",
            "an              0.003101969849643372\n",
            "as              0.003091750313569432\n",
            "would           0.0030570038909180346\n",
            "loss            0.002869645729562461\n",
            "bank            0.002837624516530781\n",
            "not             0.0027797138121117854\n",
            "inc             0.002655716774414642\n",
            "company         0.002526950619882993\n",
            "1986            0.002479259451537938\n",
            "which           0.002444513028886541\n",
            "but             0.0023375485513126315\n",
            "are             0.002312340362330245\n",
            "this            0.0022653304963901194\n",
            "shr             0.002259198774745755\n",
            "corp            0.0022142328160204175\n",
            "were            0.002120213084140166\n",
            "have            0.002113400060090872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vcXz6c8zJbj"
      },
      "source": [
        "**1.2 Extract bigrams from training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feVDSG1Yz7aX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f183a18a-8257-43ee-ad22-d9c5678ec375"
      },
      "source": [
        "#TODO: extract bigrams from training data, \n",
        "#output top 50 most frequent bigrams and their probabilities\n",
        "\n",
        "\n",
        "bigrams = [bg for sent in sentences for bg in nltk.bigrams(sent.split())]\n",
        "\n",
        "bigram_fdist = nltk.FreqDist(bigrams)\n",
        "\n",
        "bg_count = len(bigrams)\n",
        "for bg, freq in bigram_fdist.most_common(50): \n",
        "    probability = freq / unigram_fdist[bg[0]] # P(bg[1]|bg[0]) = count(bg) / count(bg[0])\n",
        "    print(f\"{str(bg):<30} {probability}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<s>', 'the')                 0.18155\n",
            "('said', '</s>')               0.31503521982025745\n",
            "('of', 'the')                  0.18675960107994932\n",
            "('in', 'the')                  0.22731455917540955\n",
            "('mln', 'dlrs')                0.26262921217076446\n",
            "('said', 'it')                 0.1644401263055623\n",
            "('said', 'the')                0.144887053679864\n",
            "('mln', 'vs')                  0.1872335455235648\n",
            "('for', 'the')                 0.20114251352976548\n",
            "('cts', 'vs')                  0.40104947526236884\n",
            "('the', 'company')             0.03596182085168869\n",
            "('to', 'the')                  0.06867124856815579\n",
            "('he', 'said')                 0.4872538860103627\n",
            "('will', 'be')                 0.34800301431801056\n",
            "('on', 'the')                  0.22649339303265262\n",
            "('billion', 'dlrs')            0.31903302689819546\n",
            "('the', 'us')                  0.026784140969162994\n",
            "('<s>', 'it')                  0.030116666666666667\n",
            "('cts', 'net')                 0.26401799100449774\n",
            "('to', 'be')                   0.04604810996563574\n",
            "('with', 'the')                0.24925723221266616\n",
            "('dlrs', 'in')                 0.13147076096988003\n",
            "('and', 'the')                 0.05901104322416607\n",
            "('by', 'the')                  0.21428571428571427\n",
            "('it', 'said')                 0.14524765729585007\n",
            "('pct', 'of')                  0.15085645597515776\n",
            "('vs', 'loss')                 0.12461300309597523\n",
            "('<s>', 'shr')                 0.02355\n",
            "('in', 'a')                    0.05190502484815019\n",
            "('dlrs', '</s>')               0.11603322426378052\n",
            "('<s>', 'he')                  0.022183333333333333\n",
            "('at', 'the')                  0.18107734806629835\n",
            "('that', 'the')                0.17760196426135588\n",
            "('from', 'the')                0.16714360041623308\n",
            "('<s>', 'in')                  0.020916666666666667\n",
            "('company', 'said')            0.32299811269884066\n",
            "('net', '</s>')                0.20615492218919393\n",
            "('of', 'its')                  0.03129649016474737\n",
            "('of', 'a')                    0.030469998347016365\n",
            "('would', 'be')                0.24203253844439493\n",
            "('year', '</s>')               0.15795813883451287\n",
            "('last', 'year')               0.3443818362611866\n",
            "('pct', '</s>')                0.10397676049283783\n",
            "('<s>', 'but')                 0.017283333333333335\n",
            "('a', 'share')                 0.04149276712994854\n",
            "('pct', 'in')                  0.10157267354502654\n",
            "('to', 'a')                    0.028894616265750288\n",
            "('<s>', 'a')                   0.016433333333333335\n",
            "('dlrs', 'vs')                 0.08196996392314791\n",
            "('inc', 'said')                0.23627501282709082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-azipsMze-o"
      },
      "source": [
        "**1.3 Extract trigrams from training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN1MtrxL0fx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbeed038-f1fb-4079-b34e-99c21d701a61"
      },
      "source": [
        "#TODO: extract trigrams from training data, \n",
        "#output top 50 most frequent trigrams and their probabilities\n",
        "#this will require re-padding each sentence with <s><s> at the beginning \n",
        "#that is, append extra <s> to the beginning of each sentence\n",
        "\n",
        "\n",
        "trigrams = [tg for sent in sentences for tg in nltk.trigrams([\"<s>\"] + sent.split())]\n",
        "\n",
        "trigram_fdist = nltk.FreqDist(trigrams)\n",
        "\n",
        "# condition_pairs = (((w0, w1), w2) for w0, w1, w2 in trigrams)\n",
        "# trigram_probabilities = nltk.ConditionalFreqDist(condition_pairs)\n",
        "\n",
        "tg_count = len(trigrams)\n",
        "num_of_sents = len(sentences)\n",
        "for tg, freq in trigram_fdist.most_common(50): \n",
        "    if tg[:2] == (\"<s>\",\"<s>\"):\n",
        "        probability = freq / num_of_sents\n",
        "    else: \n",
        "        probability = freq / bigram_fdist[(tg[0],tg[1])] # P(tg[2]|tg[0],tg[1]) = count(tg) / count(tg[0],tg[1])\n",
        "    \n",
        "    print(f\"{str(tg):<35} {probability}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<s>', '<s>', 'the')               0.18155\n",
            "('<s>', '<s>', 'it')                0.030116666666666667\n",
            "('<s>', '<s>', 'shr')               0.02355\n",
            "('<s>', '<s>', 'he')                0.022183333333333333\n",
            "('<s>', '<s>', 'in')                0.020916666666666667\n",
            "('he', 'said', '</s>')              0.4802211824755423\n",
            "('the', 'company', 'said')          0.4593711719069008\n",
            "('<s>', 'the', 'company')           0.09547415771596438\n",
            "('<s>', '<s>', 'but')               0.017283333333333335\n",
            "('<s>', '<s>', 'a')                 0.016433333333333335\n",
            "('<s>', 'it', 'said')               0.46873270614277807\n",
            "('<s>', 'he', 'said')               0.5507137490608565\n",
            "('mln', 'dlrs', '</s>')             0.1625528129864354\n",
            "('mln', 'dlrs', 'in')               0.15677118078719146\n",
            "('<s>', '<s>', 'us')                0.010866666666666667\n",
            "('qtr', 'net', '</s>')              0.9470499243570348\n",
            "('inc', 'said', 'it')               0.6775244299674267\n",
            "('pct', 'of', 'the')                0.3944223107569721\n",
            "('said', 'it', 'has')               0.14524864598719842\n",
            "('mln', 'dlrs', 'of')               0.12853013119857684\n",
            "('corp', 'said', 'it')              0.7065637065637066\n",
            "('the', 'united', 'states')         0.8872549019607843\n",
            "('cts', 'vs', 'loss')               0.20224299065420562\n",
            "('<s>', '<s>', 'this')              0.0089\n",
            "('<s>', '<s>', 'they')              0.008616666666666667\n",
            "('mln', 'avg', 'shrs')              0.9688715953307393\n",
            "('<s>', '<s>', 'we')                0.008216666666666667\n",
            "('it', 'said', '</s>')              0.31994733377221857\n",
            "('the', 'end', 'of')                0.967479674796748\n",
            "('dlrs', 'a', 'share')              0.5453474676089517\n",
            "('cts', 'net', 'loss')              0.23850085178875638\n",
            "('cts', 'a', 'share')               0.8137651821862348\n",
            "('billion', 'dlrs', 'in')           0.21344717182497333\n",
            "('<s>', 'the', 'bank')              0.035986413292940424\n",
            "('is', 'expected', 'to')            0.8631346578366446\n",
            "('the', 'bank', 'of')               0.42295081967213116\n",
            "('said', 'in', 'a')                 0.5624082232011748\n",
            "('sources', 'said', '</s>')         0.5766312594840668\n",
            "('<s>', '<s>', 'however')           0.0062833333333333335\n",
            "('said', 'it', 'will')              0.09133431806991629\n",
            "('securities', 'and', 'exchange')   0.9179487179487179\n",
            "('and', 'exchange', 'commission')   0.964769647696477\n",
            "('the', 'securities', 'and')        0.8004750593824228\n",
            "('it', 'said', 'the')               0.2185648452929559\n",
            "('said', 'it', 'is')                0.08099458394879369\n",
            "('dlrs', 'per', 'share')            0.5795053003533569\n",
            "('he', 'said', 'the')               0.13951509995746492\n",
            "('<s>', '<s>', 'qtly')              0.005433333333333333\n",
            "('nine', 'mths', 'shr')             0.6303501945525292\n",
            "('<s>', 'shr', 'loss')              0.22859164897381457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnRBwKOz0n0t"
      },
      "source": [
        "# **Task 2 - Random sentence generator using n-grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4av4l3fGc53y"
      },
      "source": [
        "# randomly generate 10 sentences using each language model, the maximum length of each sentence is set to 30\n",
        "# suppose all 10 sentences are stored in sent_list, print_sentences() is to print all the sentences\n",
        "# Feel free to make change to the code in this cell if needed. \n",
        "num_sentences = 10\n",
        "max_sent_len = 30\n",
        "random.seed()\n",
        "\n",
        "def print_sentences(sent_list):\n",
        "    for sent, prob in sent_list:\n",
        "        for word in sent:\n",
        "            print(word,end=\" \")\n",
        "        print(\"\\nprobability of: \", prob)\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "def ngram_random_sentence(model, seed, n=50):\n",
        "    # how far are you looking back from current word; bigram -> 1, trigram -> 2, etc.\n",
        "    hist_size = len(seed) * -1\n",
        "\n",
        "    # max number of tries to get a value that is not <UNK>\n",
        "    max_tries = 100\n",
        "\n",
        "    # probability of the sentence\n",
        "    probability = 1.0\n",
        "\n",
        "    sentence = seed\n",
        "\n",
        "    while len(sentence) <= max_sent_len:\n",
        "        # hist_size number of words before current word\n",
        "        given = tuple(w for w in sentence[hist_size:])\n",
        "\n",
        "        # get n most probable next words from the model[given], P(word|given)\n",
        "        most_probable = heapq.nlargest(n, model[given].items(), key=operator.itemgetter(1))\n",
        "\n",
        "        # choose one (word, probability) pair\n",
        "        choice = random.choice(most_probable)\n",
        "\n",
        "        tries = 0\n",
        "        while choice[0] == \"<UNK>\" and tries <= max_tries:\n",
        "            choice = random.choice(most_probable)\n",
        "            most_probable = heapq.nlargest(n * 2, model[given].items(), key=operator.itemgetter(1))\n",
        "            tries += 1\n",
        "\n",
        "        sentence.append(choice[0])\n",
        "        probability *= choice[1]\n",
        "\n",
        "        if sentence[-1] == \"</s>\":\n",
        "            break\n",
        "\n",
        "\n",
        "    if not sentence[-1] == \"</s>\":\n",
        "        sentence.append(\"</s>\")\n",
        "    \n",
        "    return sentence, probability\n",
        "\n",
        "def old_ngram_random_sentence(model, seed):\n",
        "    # how far are you looking back from current word; bigram -> 1, trigram -> 2, etc.\n",
        "    hist_size = len(seed) * -1\n",
        "    probability = 1.0\n",
        "    sentence = seed\n",
        "\n",
        "    while len(sentence) <= max_sent_len:\n",
        "        r = random.random()\n",
        "        accum = 0.0\n",
        "\n",
        "        # hist_size number of words before current word\n",
        "        given = tuple(w for w in sentence[hist_size:])\n",
        "\n",
        "\n",
        "        for word in model[given]: \n",
        "            accum += model[given][word] # P(word|given)\n",
        "\n",
        "            if accum > r:\n",
        "                probability *= model[given][word] # P(word|given)\n",
        "                sentence.append(word)\n",
        "                break\n",
        "\n",
        "        if sentence[-1] == \"</s>\":\n",
        "            break\n",
        "\n",
        "\n",
        "    if not sentence[-1] == \"</s>\":\n",
        "        sentence.append(\"</s>\")\n",
        "    \n",
        "    return sentence, probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMYjER4p0yQI"
      },
      "source": [
        "**2.1 Generate 10 sentences (with their probabilities) using the unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcKap9mX_Uxr"
      },
      "source": [
        "unigram_model = {}\n",
        "for ug, freq in unigram_fdist.most_common(200): \n",
        "    unigram_model[ug] = freq/unigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0phnPfL0_1G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ef95c6-a097-4ec2-b796-24c2e5f60eeb"
      },
      "source": [
        "unigram_sentences = []\n",
        "\n",
        "def unigram_random_sentence(unigram_model):\n",
        "    probability = unigram_model[\"<s>\"]\n",
        "\n",
        "    sentence = [\"<s>\"]\n",
        "\n",
        "    while len(sentence) <= max_sent_len:\n",
        "        # choose one (word, probability) pair\n",
        "        choice = random.choice(list(unigram_model.items()))\n",
        "\n",
        "        while choice[0] == \"<s>\":\n",
        "            choice = random.choice(list(unigram_model.items()))\n",
        "        \n",
        "        sentence.append(choice[0])\n",
        "        probability *= choice[1]\n",
        "\n",
        "        if sentence[-1] == \"</s>\":\n",
        "            break\n",
        "\n",
        "    if not sentence[-1] == \"</s>\":\n",
        "        sentence.append(\"</s>\")\n",
        "        probability *= unigram_model[\"</s>\"]\n",
        "\n",
        "    return sentence, probability\n",
        "\n",
        "\n",
        "#generate the sentences\n",
        "for index in range(0,num_sentences):\n",
        "    test, probability = unigram_random_sentence(unigram_model)\n",
        "\n",
        "    unigram_sentences.append((test,probability))\n",
        "    \n",
        "#TODO: generate 10 sentences using the unigram model. Suppose maximum sentence length = 30\n",
        "    \n",
        "print_sentences(unigram_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> was officials expected industry 1 loss yen other not 25 now </s> \n",
            "probability of:  6.339544306575053e-36\n",
            "\n",
            "\n",
            "\n",
            "<s> per also some exports banks earnings world we prices 1985 other because american month cts international rose be after net offer meeting export current stock international board month if money </s> \n",
            "probability of:  1.5864586864867645e-92\n",
            "\n",
            "\n",
            "\n",
            "<s> may bank week is officials west earnings dlr the major american of as rise net export officials common has we this it sale revs agreement loan world capital us trade </s> \n",
            "probability of:  9.293000458771171e-88\n",
            "\n",
            "\n",
            "\n",
            "<s> by sale sale end against january issue increase all japanese told foreign rose 4 made 20 its increase for japan earlier dlrs were west nine dlr offer co price other </s> \n",
            "probability of:  5.29308591647337e-92\n",
            "\n",
            "\n",
            "\n",
            "<s> price corp have interest 1985 revs foreign board while more total increase american were finance made us this due if made countries march at under were inc dlr foreign this </s> \n",
            "probability of:  7.335563968431593e-91\n",
            "\n",
            "\n",
            "\n",
            "<s> that tax been oper dollar are dollar vs officials yen corp were offer production shr six current share as which agreement 30 could shrs economic finance debt share when would </s> \n",
            "probability of:  3.3026377699531222e-90\n",
            "\n",
            "\n",
            "\n",
            "<s> inc ltd rates that as all group share has january banks sales quarter three other six but any meeting under that money said are to ltd be up has trade </s> \n",
            "probability of:  7.175293615201033e-86\n",
            "\n",
            "\n",
            "\n",
            "<s> for four while issue industry had official at rate 10 were rate sources when 31 due quarter officials months two growth industry was may march with if because over finance </s> \n",
            "probability of:  1.0930152275692807e-91\n",
            "\n",
            "\n",
            "\n",
            "<s> world all finance dlr an but finance dollar financial week 12 as per they vs any per end dlr have are through unit 2 record issue </s> \n",
            "probability of:  3.918247216086374e-80\n",
            "\n",
            "\n",
            "\n",
            "<s> west net inc credit industry be through 4 officials 12 other record issue 4 loss shrs officials months shares first for by west due from an new loan three on </s> \n",
            "probability of:  6.68833117227134e-90\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K_2ZbtN1Ol2"
      },
      "source": [
        "**2.2 Generate 10 sentences (with their probabilities) using the bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf4bdFDKT0_F"
      },
      "source": [
        "bigram_model = train_ngram(bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIh6fxVp1Vjb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542d6256-2e18-4f48-b4b6-2f3cf7d42ad1"
      },
      "source": [
        "\n",
        "bigram_sentences = []\n",
        "\n",
        "# for word in bigram_model[\"liberty\",]:\n",
        "#     print(word)\n",
        "\n",
        "# print()\n",
        "\n",
        "# test, probability = ngram_random_sentence(bigram_model, [\"<s>\"])\n",
        "\n",
        "# print()\n",
        "# print(test)\n",
        "# print(probability)\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,num_sentences):\n",
        "    test, probability = ngram_random_sentence(bigram_model, [\"<s>\"])\n",
        "\n",
        "    bigram_sentences.append((test,probability))\n",
        "    \n",
        "\n",
        "#ToDo: generate 10 sentences using the bigram model. Suppse maximum sentence length = 30\n",
        "\n",
        "print_sentences(bigram_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> as many concessions on tuesday morning delegates saying more on tuesday they agreed yesterday he raised were seen whites market from 25 largest in quarter sales fell 178 154 152 </s> \n",
            "probability of:  7.377269879526369e-67\n",
            "\n",
            "\n",
            "\n",
            "<s> earlier a 1 10 to around march 27 after the group sales gained 26 cts 25 week fell 22 cts higher to make some progress industrial corp to seek protection </s> \n",
            "probability of:  9.593528722286043e-65\n",
            "\n",
            "\n",
            "\n",
            "<s> uk one or if that will receive 92 since november 1982 before the companys capital markets that it expects from 11 elections due for management to be an export quotas </s> \n",
            "probability of:  6.64771642855045e-59\n",
            "\n",
            "\n",
            "\n",
            "<s> the countrys creditors there had to improve says bank was announced program acreage have taken care a definitive documentation japsper said earlier due september 1 0 1 1990 said sales </s> \n",
            "probability of:  6.306728320417958e-62\n",
            "\n",
            "\n",
            "\n",
            "<s> japanese trading of us official who had a 10 25 record in washington said any possible future earnings after mondays record </s> \n",
            "probability of:  9.556897558456888e-47\n",
            "\n",
            "\n",
            "\n",
            "<s> on september a one arbitrager acknowledged the end december but that there long government owned bank borrowings than 200 years to purchase valued its 1986 year after june four ships </s> \n",
            "probability of:  7.220375709170255e-65\n",
            "\n",
            "\n",
            "\n",
            "<s> analysts today has changed hands was unable to do just started in oil exports as chief uk firms seeking 25 week has received shareholder approval buy level by four cargoes </s> \n",
            "probability of:  1.0919564685960213e-64\n",
            "\n",
            "\n",
            "\n",
            "<s> as scheduled for 1986 against prt relief </s> \n",
            "probability of:  1.487204619648738e-14\n",
            "\n",
            "\n",
            "\n",
            "<s> brazil were reduced supplies and development activities to maintain prices to reduce revenues came tried dauster a two year when final figure likely over what they believe what producers along </s> \n",
            "probability of:  6.576276466170701e-65\n",
            "\n",
            "\n",
            "\n",
            "<s> it intends to maintain self storage facilties </s> \n",
            "probability of:  5.537609772799519e-13\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUdZ4utZ1kIE"
      },
      "source": [
        "**2.3 Generate 10 sentences (with their probabilities) using the trigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heLN3b9oSEQ0"
      },
      "source": [
        "trigram_model = train_ngram(trigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgL2-pFw1kIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def2a69c-3ac0-4fc9-8da9-634c262847ea"
      },
      "source": [
        "trigram_sentences = []\n",
        "\n",
        "# for word in trigram_model[(\"<s>\",\"liberty\")]:\n",
        "#     print(word)\n",
        "\n",
        "\n",
        "# test, probability = ngram_random_sentence(trigram_model, [\"<s>\",\"<s>\"])\n",
        "\n",
        "# print()\n",
        "# print(test)\n",
        "# print(probability)\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,num_sentences):\n",
        "    test, probability = ngram_random_sentence(trigram_model, [\"<s>\",\"<s>\"])\n",
        "\n",
        "    trigram_sentences.append((test,probability))\n",
        "\n",
        "#ToDo: generate 10 sentences using the trigram model. Suppse maximum sentence length = 30\n",
        "    \n",
        "print_sentences(trigram_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> <s> analysts computer industry could charge foreign producers are annoyed the buffer </s> \n",
            "probability of:  4.3785214258567445e-11\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> japanese buyers of non metal mineral ores </s> \n",
            "probability of:  1.1022927689594354e-09\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> but it is likely to happen any time before some operators try to end august 1986 when gross domestic product will be used simultaneously merrill said it does points </s> \n",
            "probability of:  9.010259212661037e-30\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the report indicated that they soviets will buy the gwinnett daily news severance charges of about 21 mln in principal to foreign banks have largely achieved what we want </s> \n",
            "probability of:  4.1034456149783543e-29\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> its commercial paper </s> \n",
            "probability of:  1.3687600644122387e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> we expect natural gas subsidiary c </s> \n",
            "probability of:  8.92857142857143e-09\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> we pointed out its reaction to what most of first priority for agriculture workers would push to coordinate repayment of all corporate ratings changes during the balance on july </s> \n",
            "probability of:  1.8350393263359682e-27\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> 1986 oper shr 60 cts net 617000 vs 604000 revs 1889000 vs 1920000 </s> \n",
            "probability of:  2.6889999637945364e-12\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> uk domestic wheat markets since such a project which might provide gencorp shareholders and regulatory and permitting issues that limited the decline this week for meetings between the european </s> \n",
            "probability of:  6.281708709611551e-25\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> this compared to 26 top officers of pw korth and painewebber along with an eight billion marks </s> \n",
            "probability of:  1.6053778426428634e-17\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foG7qAwPsW7M"
      },
      "source": [
        "#**Task 3 - Deal with unseen words and smoothing**\n",
        "\n",
        "The current n-gram models can’t predict the probability of a given sentence if the sentence contains an unknown token or bi-/tri-gram simply because it’s not included in the ngram model from which it looks up corrensponding probability. \n",
        "\n",
        "The basic idea of smoothing is to take some probability mass from the words seen during training and reallocate it to words not seen during training. Assume we have decided how much probability mass to reallocate, according to some smoothing scheme. The question is, how do we decide how to allocate this probability mass among unknown words, when we don't even know how many unknown words there are? (No fair peeking at the test data!)\n",
        "\n",
        "There are multiple approaches, but no perfect solution. (This is an opportunity for you to experiment and innovate.) A straightforward and widely-used approach is to assume a special token `<UNK>` which represents (an equivalence class of) all unknown words. All of the reallocated probability mass is assigned to this special token, and any unknown word encountered during testing is treated as an instance of this token.\n",
        "\n",
        "This approach has the shortcoming that it treats all unknown words alike, that is, it will assign the same probability to any unknown word. You might think that it's possible to do better than this. Here are two unknown words you might encounter, say, on the internet: \"flavodoxin\" and \"B000EQHXQY\". Intuitively, which should be considered more probable? What kind of knowledge are you applying? Do you think a machine could make the same judgment?\n",
        "\n",
        "A paper by Efron & Thisted, Estimating the number of unseen species: How many words did Shakespeare know?, addresses related issues.\n",
        "\n",
        "In the following, you'll need do two things -\n",
        "- replace all the low-frequency words (frequency = 1, 2, 5, or some other value) in the training data with the token `<UNK>`. You may try 1 to start. \n",
        "- use add-lambda smoothing (lambda = 1 in add-one smoothing) where you can let lambda be 1 or 0.001. You may try .001 to start. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdnSESdZ6KgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "668a342f-0294-4d81-9767-a98846e7f940"
      },
      "source": [
        "#TODO: replace all the low-frequent words in the training data with the token <UNK>.\n",
        "# the rest will be similar to the part right preceeding part 1. \n",
        "# also report the size of the vocabulary and set the value of lambda to prepare for add-lambda smoothing. \n",
        "lambda_value = 0.00001\n",
        "\n",
        "def replace_low_freq(sentence):\n",
        "    low_freq = 1\n",
        "\n",
        "    words_UNK = [word if unigram_fdist[word] > low_freq else \"<UNK>\" for word in sentence.split()]\n",
        "  \n",
        "    return \" \".join(words_UNK)\n",
        "\n",
        "sentences_UNK = [replace_low_freq(sent) for sent in sentences]\n",
        "\n",
        "for sent in sentences_UNK[0:10]:\n",
        "    print(sent, end=\"\\n\")\n",
        "\n",
        "\n",
        "def train_ngram_smoothed(ngrams, vocab):\n",
        "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "    # vocab = len(set(ngrams)) # make vocab number of N-1 grams\n",
        "\n",
        "    for ng in ngrams:\n",
        "        model[ng[:-1]][ng[-1]] += 1  # [(w1, w2, ... , wn-1)][wn]\n",
        "    \n",
        "    total_count = 0      \n",
        "    for hist in model: # hist = (w1, w2, ... , wn-1)\n",
        "        total_count = float(sum(model[hist].values()))\n",
        "        # if \"<UNK>\" in hist:\n",
        "        #     total_count = 0\n",
        "        for w in model[hist]: # w = wn\n",
        "            count = model[hist][w]\n",
        "            # if w == \"<UNK>\":\n",
        "            #     count = 0\n",
        "            model[hist][w] = (count + lambda_value)/(total_count + lambda_value * vocab)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> liberty all star usa sets initial payout </s>\n",
            "<s> we are being accused of not implementing this agreement </s>\n",
            "<s> entregrowth closed at 135 dlrs and options at 55 cents </s>\n",
            "<s> usda forecast south african 1986 87 corn exports at 210 mln tonnes vs 300 mln tonnes last month and 1985 86 exports at 275 mln tonnes vs 275 mln tonnes last month </s>\n",
            "<s> <UNK> issued capital will be <UNK> mln shares of which 63 pct will be held by nbh after 89 mln are issued to shareholders to raise 196 mln dlrs it said </s>\n",
            "<s> the april 6 sale to be evenly divided between the three and six month issues will result in a paydown of 165 billion dlrs as maturing bills total 1485 billion dlrs </s>\n",
            "<s> waste managements tender offer announced before the opening today expires march 25 </s>\n",
            "<s> he earlier estimated the damage from the us raid at about 500 mln dlrs </s>\n",
            "<s> brougher bigi to sell 40 pct of subsidiary </s>\n",
            "<s> that was not the case two years ago </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf4gO-qs68NV"
      },
      "source": [
        "**3.1 Extract smoothed unigrams from training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzEHmPtI7FKO"
      },
      "source": [
        "#Todo: extract the smoothed unigram model. same as part 1.1 except for the smoothed probability\n",
        "unigrams_UNK = [ug for sent in sentences_UNK for ug in sent.split()]\n",
        "unigrams_count_UNK = len(unigrams_UNK)\n",
        "unigrams_UNK_vocab = len(set(unigrams_UNK))\n",
        "\n",
        "unigram_fdist_UNK = nltk.FreqDist(unigrams_UNK)\n",
        "\n",
        "unigram_model_smoothed = {}\n",
        "for ug, freq in unigram_fdist_UNK.most_common(200): \n",
        "    if ug == \"<UNK>\":\n",
        "        freq = 0\n",
        "    \n",
        "    unigram_model_smoothed[ug] = (freq + lambda_value)/(unigrams_count_UNK + lambda_value * unigrams_UNK_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNrT5Jlv7JeM"
      },
      "source": [
        "**3.2 Extract smoothed bigrams from training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HEUcwkf7NK9"
      },
      "source": [
        "#Todo: extract the smoothed bigram model. same as part 1.2 except for the smoothed probability\n",
        "\n",
        "bigrams_UNK = [bg for sent in sentences_UNK for bg in nltk.bigrams(sent.split())]\n",
        "bigrams_UNK_vocab = len(set(bigrams_UNK))\n",
        "\n",
        "bigram_model_smoothed = train_ngram_smoothed(bigrams_UNK,unigrams_UNK_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei61QyJQ7RE4"
      },
      "source": [
        "**3.3 Extract smoothed trigrams from training data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVyF6VKP7YQX"
      },
      "source": [
        "#Todo: extract the smoothed trigram model, same as part 1.3 except for the smoothed probability\n",
        "\n",
        "#re-padding each sentence with <s><s> at the beginning \n",
        "#that is, append extra <s> to the beginning of each sentence\n",
        "\n",
        "trigrams_UNK = [tg for sent in sentences_UNK for tg in nltk.trigrams([\"<s>\"] + sent.split())]\n",
        "trigrams_UNK_vocab = len(set(bigrams_UNK))\n",
        "\n",
        "trigram_model_smoothed = train_ngram_smoothed(trigrams_UNK,bigrams_UNK_vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnm5kzNx7oID"
      },
      "source": [
        "# **Task 4 - Random sentence generator using smoothed n-grams**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAnWYvFZ7zoQ"
      },
      "source": [
        "**4.1 Generate 10 sentences (with probabilities) using the smoothed unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRc7BTmM7zoR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0546a6bb-a875-4e0c-a036-1307ad880f8f"
      },
      "source": [
        "unigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for index in range(0,num_sentences):\n",
        "    test, probability = unigram_random_sentence(unigram_model_smoothed)\n",
        "\n",
        "    unigram_sentences.append((test,probability))\n",
        "    \n",
        "#TODO: generate 10 sentences using the smoothed unigram model. Suppose maximum sentence length = 30\n",
        "    \n",
        "print_sentences(unigram_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> if profit loan exchange company export while we west through about tax this japan into months loan securities japanese stock nine yen share japan new 4 were up told by </s> \n",
            "probability of:  9.456286023975352e-93\n",
            "\n",
            "\n",
            "\n",
            "<s> than after its shr to rate inc and banks government expected trade from foreign billion could had or common between 30 we because been but american earnings finance not made </s> \n",
            "probability of:  2.486760097080471e-87\n",
            "\n",
            "\n",
            "\n",
            "<s> has had was rates time more between told at while vs dlr four tonnes been investment three because group world government foreign capital financial february spokesman after share dollar and </s> \n",
            "probability of:  3.1754295172463096e-90\n",
            "\n",
            "\n",
            "\n",
            "<s> company at net world international months this rate years would price bank cash tax dollar for major at its three 30 year export profit for vs total some to or </s> \n",
            "probability of:  1.051098633164143e-84\n",
            "\n",
            "\n",
            "\n",
            "<s> exchange this over it credit february from spokesman 30 shrs offer april not <UNK> exports dollar their earnings time pay oper revs profit this company international if to or pay </s> \n",
            "probability of:  3.7021665879624976e-98\n",
            "\n",
            "\n",
            "\n",
            "<s> expected up if but agreement years would 12 loss production new four against interest it nine economic april years between be vs tonnes shr years march japanese net but there </s> \n",
            "probability of:  7.163163402566741e-89\n",
            "\n",
            "\n",
            "\n",
            "<s> unit finance two against 2 over on 2 due meeting first sales would in share sources if 4 two as expected an into up 12 exchange west by it finance </s> \n",
            "probability of:  2.6101323811505555e-89\n",
            "\n",
            "\n",
            "\n",
            "<s> pay west world last 12 30 exchange dlrs first this months foreign sales four spokesman he tax said five will into and securities been are earlier would other added first </s> \n",
            "probability of:  8.020346514393065e-89\n",
            "\n",
            "\n",
            "\n",
            "<s> as by international 15 due today sales than about vs april three also price major 1985 all may all rate major dlrs west three sale that we sources shares interest </s> \n",
            "probability of:  1.0667035062627538e-89\n",
            "\n",
            "\n",
            "\n",
            "<s> growth through we but 31 made officials some have growth he now japan offer we exports made to in co six company to us from finance 1986 profit financial 10 </s> \n",
            "probability of:  5.553989426974862e-88\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_8mupAB77Fp"
      },
      "source": [
        "**4.2 Generate 10 sentences (with probabilities) using the smoothed bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIYeKErZ77Fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2caf728b-58f6-483c-df1f-d2e4959a49f6"
      },
      "source": [
        "bigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,num_sentences):\n",
        "    test, probability = ngram_random_sentence(bigram_model_smoothed, [\"<s>\"])\n",
        "\n",
        "    bigram_sentences.append((test,probability))\n",
        "\n",
        "#ToDo: generate 10 sentences using the smoothed bigram model. Suppse maximum sentence length = 30\n",
        "\n",
        "print_sentences(bigram_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> total us oil traders and the acquisition that he still being placed its economy continues to be for which fell 02 nigeria and industry output expected improvement in october 14 </s> \n",
            "probability of:  3.4446681060629033e-60\n",
            "\n",
            "\n",
            "\n",
            "<s> an indicated as at five g under federal trade policy decisions bosworth said i did succeed we know who has also rose to go star gets 52 pct higher us </s> \n",
            "probability of:  7.576873342923105e-61\n",
            "\n",
            "\n",
            "\n",
            "<s> i see these matters might come different origin due between seven pretax net assets 45 tonnes 1986 a group including those issued earlier new crop report it believes subsequent repayments </s> \n",
            "probability of:  8.363363096616448e-65\n",
            "\n",
            "\n",
            "\n",
            "<s> japan were likely mean he cited tax income tax rate currently provides </s> \n",
            "probability of:  8.561472143295514e-29\n",
            "\n",
            "\n",
            "\n",
            "<s> national wildlife in response she sees second qtr excludes 625000 dlr contract expiry of japan officials point purolator courier companies operating next autumn intervention bd says trade data voice his </s> \n",
            "probability of:  2.6777721531215295e-59\n",
            "\n",
            "\n",
            "\n",
            "<s> if usda general mills said sales gain a number far refused a very much quicker than many countries was needed injection molding custom power with this move over 200 corporations </s> \n",
            "probability of:  9.729230489242165e-63\n",
            "\n",
            "\n",
            "\n",
            "<s> total number cash settlement futures exchange contracts amid signs from 7 13 of trade weighted index also offered a senior us flag would become payable november after todays economic policy </s> \n",
            "probability of:  2.1588915833320663e-61\n",
            "\n",
            "\n",
            "\n",
            "<s> we had reached over 15 1997 8 while production figures have some time last traded down two firms study contained caesium levels if this years if you acted on sale </s> \n",
            "probability of:  7.433034095437834e-64\n",
            "\n",
            "\n",
            "\n",
            "<s> shr three consecutive trading profits of 1987 target record profits this issue of us official sees opec conference also help modernise but still receive new accord said however dome has </s> \n",
            "probability of:  8.923573471998281e-64\n",
            "\n",
            "\n",
            "\n",
            "<s> us bank is likely timing was that some industry to have more important politicians have decided epac should increase by president has about 12 days at prices but in april </s> \n",
            "probability of:  2.792928362552215e-62\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28y2NZ348BJC"
      },
      "source": [
        "**4.3 Generate 10 sentences (with probabilities) using the smoothed trigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrKjSnLv8BJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016a1244-5955-4381-e1d3-c6d051df3c44"
      },
      "source": [
        "trigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,num_sentences):\n",
        "    test, probability = ngram_random_sentence(trigram_model_smoothed, [\"<s>\",\"<s>\"])\n",
        "\n",
        "    trigram_sentences.append((test,probability))\n",
        "\n",
        "#ToDo: generate 10 sentences using the smoothed trigram model. Suppse maximum sentence length = 30\n",
        "    \n",
        "print_sentences(trigram_sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> <s> us home resales up in court settlement </s> \n",
            "probability of:  1.49967398887116e-10\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> national business review newspaper published in june 10 </s> \n",
            "probability of:  3.417181547416186e-12\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> first connecticut has assets of this has increasingly failed to bridge the funding gap </s> \n",
            "probability of:  1.9402094455712195e-19\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> american powdered makes metal parts for various sept dec same section 125 pct while real imports had risen almost three dollars below a target companys stock that will have </s> \n",
            "probability of:  5.650583488141795e-31\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> new homes </s> \n",
            "probability of:  3.866469509864546e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> for discontinued compressor operations vs loss 35 cts net 1523000 vs 2557000 dlrs in new manufacturing plant targeted for sale and or canadian transshipment points by september 20 the </s> \n",
            "probability of:  3.11766149234365e-33\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> we may not occur but it hasnt already agreed to stimulate growth and huge trade imbalances it welcomed the february index was down 1 2 marks each to purchase </s> \n",
            "probability of:  2.4018995270740245e-34\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> there appears to tolerate by saying that tehran officially announced fell to 234 pct below price levels will have little if any more stock and debt leverage of the </s> \n",
            "probability of:  1.030146567810822e-34\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> moodys affirms chrysler c bid </s> \n",
            "probability of:  8.687781618007917e-09\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> last year but 115 pct to 25 dlrs below saudi arabias state owned capital of b 1 convertible debt due to last weeks election now looks likely to raise </s> \n",
            "probability of:  6.132009647696723e-36\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JYmPlq_d1Xl"
      },
      "source": [
        "# **Task 5 - Evaluate your language models**\n",
        "\n",
        "Computing the perplexity of the test data for the three smoothed language models. An introduction to perplexity can be found on page 8 of [Chapter 3](https://web.stanford.edu/~jurafsky/slp3/3.pdf) of Jurasky and Martin's book. \n",
        "\n",
        "As an example, for bigram, the perplexity of a corpus W with N words is calculated as shown [here](https://drive.google.com/file/d/1r-WGzQxKiecJ_vlGqiYEryLkZajEMQTO/view?usp=sharing).\n",
        "\n",
        "In the following, report the perplexities for the 3 smoothed language models on the test data set. \n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXhPyKczFuhl"
      },
      "source": [
        "**5.0 - Process the test data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn7qMGi4yvbi"
      },
      "source": [
        "# TODO: \n",
        "# Replace all the \"unknown\" words in the test set (that is, the words that are not in the vocabulary that is exacted from the training data) with the token \"`<UNK>`\". \n",
        "# And put the test data in a list to get ready for n-gram model extraction. \n",
        "\n",
        "def replace_unknown(sentence):\n",
        "    words_UNK = [word if unigram_fdist[word] == 0 else \"<UNK>\" for word in sentence.split()]\n",
        "  \n",
        "    return \" \".join(words_UNK)\n",
        "\n",
        "\n",
        "test_sentences = load_file(test_file)\n",
        "\n",
        "test_sentences_UNK = [replace_unknown(sent) for sent in test_sentences]\n",
        "\n",
        "# i = 0\n",
        "# for sent, test_sent in zip(test_sentences, test_sentences_UNK):\n",
        "#     if \"<UNK>\" in test_sent:\n",
        "#         print(sent)\n",
        "#         print(test_sent,\"\\n\")\n",
        "#         i += 1\n",
        "\n",
        "#     if i > 10:\n",
        "#         break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOuE1E9ZGwlq"
      },
      "source": [
        "**5.1 - Calculate and report the perplexity of the unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQM4fkOXzOuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c39b2d-9e8f-44e4-91bd-072c8e6bbe0b"
      },
      "source": [
        "#TODO: calculate the perplexity of the smoothed unigram model on the test data\n",
        "# report the perplexity \n",
        "\n",
        "test_unigrams_UNK = [ug for sent in test_sentences_UNK for ug in sent.split()]\n",
        "\n",
        "ln_prob_sum = 0\n",
        "for ug in test_unigrams_UNK:\n",
        "    freq = unigram_fdist_UNK[ug]\n",
        "    # if ug == \"<UNK>\":\n",
        "    #     freq = 0\n",
        "    \n",
        "    ln_prob_sum += math.log((freq + lambda_value)/(unigrams_count_UNK + lambda_value * unigrams_UNK_vocab))\n",
        "\n",
        "\n",
        "test_unigrams_perplexity = math.exp((-1/len(test_unigrams_UNK)) * ln_prob_sum)\n",
        "\n",
        "print(test_unigrams_perplexity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "94.35421100479897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebr-51MNHC-E"
      },
      "source": [
        "**5.2 - Calculate and report the perplexity of the bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcwu6q8dzSA9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c11411-6541-482c-ff70-4e544f24d59c"
      },
      "source": [
        "#TODO: calculate the perplexity of the smoothed bigram model on the test data\n",
        "# report the perplexity \n",
        "\n",
        "test_bigrams_UNK = [bg for sent in test_sentences_UNK for bg in nltk.bigrams(sent.split())]\n",
        "test_bigrams_UNK_vocab = len(set(test_bigrams_UNK))\n",
        "\n",
        "ln_prob_sum = 0\n",
        "for bg in test_bigrams_UNK:\n",
        "    prob = bigram_model_smoothed[bg[:-1]][bg[-1]]\n",
        "    if bg[1] == \"<s>\":\n",
        "        continue\n",
        "    if prob == 0: \n",
        "        total_count = float(sum(bigram_model_smoothed[bg[:-1]].values()))\n",
        "        prob = (0 + lambda_value)/(total_count + lambda_value * unigrams_UNK_vocab)\n",
        "    ln_prob_sum += math.log(prob)\n",
        "\n",
        "test_bigrams_perplexity = math.exp((-1/len(test_bigrams_UNK)) * ln_prob_sum)\n",
        "\n",
        "print(test_bigrams_perplexity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32.234066219728085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ES6IXrHXTc"
      },
      "source": [
        "**5.3 - Calculate and report the perplexity of the trigram model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFuY7vMJzUPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c9bef58-28b4-42bd-895a-df1197958354"
      },
      "source": [
        "#TODO: calculate the perplexity of the smoothed bigram model on the test data\n",
        "# report the perplexity \n",
        "#don't forget to append extra <s> to the beginning of each sentence\n",
        "\n",
        "\n",
        "test_trigrams_UNK = [tg for sent in test_sentences_UNK for tg in nltk.trigrams([\"<s>\"] + sent.split())]\n",
        "test_trigrams_UNK_vocab = len(set(test_trigrams_UNK))\n",
        "\n",
        "ln_prob_sum = 0\n",
        "for tg in test_trigrams_UNK:\n",
        "    prob = trigram_model_smoothed[tg[:-1]][tg[-1]]\n",
        "    if tg[2] == \"<s>\" or (tg[1] == \"<s>\" and tg[2] == \"<s>\"):\n",
        "        continue\n",
        "    if prob == 0: \n",
        "        total_count = float(sum(trigram_model_smoothed[(tg[0],)].values()))\n",
        "        prob = (0 + lambda_value)/(total_count + lambda_value * bigrams_UNK_vocab)\n",
        "    \n",
        "    ln_prob_sum += math.log(prob)\n",
        "\n",
        "test_trigrams_perplexity = math.exp((-1/len(test_trigrams_UNK)) * ln_prob_sum)\n",
        "\n",
        "print(test_trigrams_perplexity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12.426922831929732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Y0clntel_P"
      },
      "source": [
        "# **Task 6 - Summary and Reflection** \n",
        "\n",
        "Summarize what you've learnt for this assignment and the challenges you've run into and tackled. Report any future work and possible improvement you would make. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xof2gMVfLvXF"
      },
      "source": [
        "I learned a lot about python dictionaries as well as the technicalities of implementing ngram language models.  That sample code I found in the link provided in Task 1 was really invaluable for helping me think through the implementations; it help me structure and generalize my code a lot.  \n",
        "\n",
        "One of the biggest difficulties I had with this assignment was figuring out the initial implementations of the different ngram models.  Once I figured out how that worked, a lot of the math and other implementations just fell into place.  I also had some trouble with the math of the smoothed models just becuase I misunderstood how we were supposed to count the \\<UNK\\> tokens.  I initially implemented the models to count tokens that contained \\<UNK\\> as having zero occurances in order for their probabilites to be really low after smoothing. This seemed to work fine until Task 5 where my smoothed models produced some really strange results for the perplexities, giving incredibly large perplexity values.  Counting \\<UNK\\> normally lead to much less strange results.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJXbbBrsDHUi"
      },
      "source": [
        "# **Extra: Most Probable Sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8vBlJWRDEbF"
      },
      "source": [
        "def print_sentences_joined(sent_list):\n",
        "    for sent, prob in sent_list:\n",
        "        print(sent)\n",
        "        print(\"probability of: \", prob)\n",
        "        print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWpwUUjEKGlR"
      },
      "source": [
        "## **Unsmoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oUM8E_-KGlU"
      },
      "source": [
        "**Unigram sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbcRJw9wKGlV",
        "outputId": "550f791d-9941-4652-be26-9d335a7ab9d3"
      },
      "source": [
        "unigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,10000):\n",
        "    test, probability = unigram_random_sentence(unigram_model)\n",
        "\n",
        "    unigram_sentences.append((test,probability))\n",
        "\n",
        "i = num_of_sents\n",
        "most_probable_unigrams = []\n",
        "base_prob = unigram_model[\"<s>\"] * unigram_model[\"</s>\"]\n",
        "for ug, freq in heapq.nlargest(12, unigram_model.items(), key=operator.itemgetter(1)):\n",
        "    if ug == \"<s>\" or ug == \"</s>\":\n",
        "        continue\n",
        "    \n",
        "    most_probable_unigrams.append(([\"<s>\",ug,\"</s>\"], freq * base_prob))\n",
        "\n",
        "\n",
        "unigram_sentences = [(\" \".join(words), prob) for words, prob in unigram_sentences + most_probable_unigrams]\n",
        "unigram_sentences_most_probable = heapq.nlargest(num_sentences, set(unigram_sentences), key=operator.itemgetter(1))\n",
        "    \n",
        "print_sentences_joined(unigram_sentences_most_probable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> </s>\n",
            "probability of:  0.0016710226810651028\n",
            "\n",
            "\n",
            "\n",
            "<s> the </s>\n",
            "probability of:  7.7529927625609e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> of </s>\n",
            "probability of:  4.132424835468951e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> to </s>\n",
            "probability of:  3.975543425383651e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> to </s>\n",
            "probability of:  3.9755434253836505e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> in </s>\n",
            "probability of:  3.092658566739602e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> and </s>\n",
            "probability of:  2.9999869645556873e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> said </s>\n",
            "probability of:  2.8122529694681258e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> a </s>\n",
            "probability of:  2.809634484394133e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> mln </s>\n",
            "probability of:  1.9494052139989764e-05\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1rDB8sbKGlY"
      },
      "source": [
        "**Bigram sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2kJo78cKGlZ",
        "outputId": "dfce51c0-0116-4155-ddba-9e73019914c9"
      },
      "source": [
        "bigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,10000):\n",
        "    test, probability = ngram_random_sentence(bigram_model, [\"<s>\"],n=2)\n",
        "\n",
        "    bigram_sentences.append((test,probability))\n",
        "\n",
        "\n",
        "bigram_sentences = [(\" \".join(words), prob) for words, prob in bigram_sentences]\n",
        "bigram_sentences_most_probable = heapq.nlargest(num_sentences, set(bigram_sentences), key=operator.itemgetter(1))\n",
        "    \n",
        "print_sentences_joined(bigram_sentences_most_probable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> it said </s>\n",
            "probability of:  0.0013780822775624668\n",
            "\n",
            "\n",
            "\n",
            "<s> the company said </s>\n",
            "probability of:  0.0006643501238030103\n",
            "\n",
            "\n",
            "\n",
            "<s> the company </s>\n",
            "probability of:  0.0005298434729746154\n",
            "\n",
            "\n",
            "\n",
            "<s> it said it said </s>\n",
            "probability of:  3.291486556938212e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> the company said it said </s>\n",
            "probability of:  1.586769917298153e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> the us agriculture department said </s>\n",
            "probability of:  8.143811253345884e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> the us agriculture committee </s>\n",
            "probability of:  9.184014946733082e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> it said it said it said </s>\n",
            "probability of:  7.86156525695098e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> it has a share </s>\n",
            "probability of:  6.029778334990044e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> it has a year </s>\n",
            "probability of:  4.0929087602791655e-07\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_60Jel9EKGla"
      },
      "source": [
        "**Trigram sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQfb3LMPKGlb",
        "outputId": "3948279e-169a-4b8a-9d40-63952af18e07"
      },
      "source": [
        "trigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,10000):\n",
        "    test, probability = ngram_random_sentence(trigram_model, [\"<s>\",\"<s>\"],n=2)\n",
        "\n",
        "    trigram_sentences.append((test,probability))\n",
        "\n",
        "\n",
        "trigram_sentences = [(\" \".join(words), prob) for words, prob in trigram_sentences]\n",
        "trigram_sentences_most_probable = heapq.nlargest(num_sentences, set(trigram_sentences), key=operator.itemgetter(1))\n",
        "    \n",
        "print_sentences_joined(trigram_sentences_most_probable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> <s> it said </s>\n",
            "probability of:  0.004516589861751152\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the company said </s>\n",
            "probability of:  0.0020537495799109857\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the company </s>\n",
            "probability of:  0.0010262692255342315\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the bank said </s>\n",
            "probability of:  0.0003509761941700712\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the bank of england said </s>\n",
            "probability of:  1.7690021842244334e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> it said the company said </s>\n",
            "probability of:  1.2972386842340124e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> it said the company </s>\n",
            "probability of:  6.482368409587638e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the bank of japan governor haruo maekawa </s>\n",
            "probability of:  1.6280381563479762e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> it said the us agriculture department said </s>\n",
            "probability of:  1.4561112739651348e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the company said it will be listed in luxembourg </s>\n",
            "probability of:  6.164617200998113e-07\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMfoJe1SJUch"
      },
      "source": [
        "## **Smoothed Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX9cS-7yEPMN"
      },
      "source": [
        "**Unigram sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfHPtZLhDaPF",
        "outputId": "13665f86-3a15-40b7-ec21-69333c576960"
      },
      "source": [
        "unigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,10000):\n",
        "    test, probability = unigram_random_sentence(unigram_model_smoothed)\n",
        "\n",
        "    unigram_sentences.append((test,probability))\n",
        "\n",
        "i = num_of_sents\n",
        "most_probable_unigrams = []\n",
        "base_prob = unigram_model_smoothed[\"<s>\"] * unigram_model_smoothed[\"</s>\"]\n",
        "for ug, freq in heapq.nlargest(12, unigram_model_smoothed.items(), key=operator.itemgetter(1)):\n",
        "    if ug == \"<s>\" or ug == \"</s>\":\n",
        "        continue\n",
        "    \n",
        "    most_probable_unigrams.append(([\"<s>\",ug,\"</s>\"], freq * base_prob))\n",
        "\n",
        "\n",
        "unigram_sentences = [(\" \".join(words), prob) for words, prob in unigram_sentences + most_probable_unigrams]\n",
        "unigram_sentences_most_probable = heapq.nlargest(num_sentences, set(unigram_sentences), key=operator.itemgetter(1))\n",
        "    \n",
        "print_sentences_joined(unigram_sentences_most_probable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> </s>\n",
            "probability of:  0.0016710221464266586\n",
            "\n",
            "\n",
            "\n",
            "<s> the </s>\n",
            "probability of:  7.75298904158908e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> of </s>\n",
            "probability of:  4.132422852684377e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> to </s>\n",
            "probability of:  3.975541517915787e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> in </s>\n",
            "probability of:  3.0926570831331966e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> in </s>\n",
            "probability of:  3.092657083133196e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> and </s>\n",
            "probability of:  2.9999855254397043e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> said </s>\n",
            "probability of:  2.8122516204807753e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> a </s>\n",
            "probability of:  2.8096331366638827e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> mln </s>\n",
            "probability of:  1.9494042792535337e-05\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCvng5ilDagG"
      },
      "source": [
        "**Bigram sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oypnAOtEO-_",
        "outputId": "25f1dbdb-f5f8-406f-a245-86b45ca008cd"
      },
      "source": [
        "bigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,10000):\n",
        "    test, probability = ngram_random_sentence(bigram_model_smoothed, [\"<s>\"],n=2)\n",
        "\n",
        "    bigram_sentences.append((test,probability))\n",
        "\n",
        "\n",
        "bigram_sentences = [(\" \".join(words), prob) for words, prob in bigram_sentences]\n",
        "bigram_sentences_most_probable = heapq.nlargest(num_sentences, set(bigram_sentences), key=operator.itemgetter(1))\n",
        "    \n",
        "print_sentences_joined(bigram_sentences_most_probable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> it said </s>\n",
            "probability of:  0.0013780328124173638\n",
            "\n",
            "\n",
            "\n",
            "<s> the company said </s>\n",
            "probability of:  0.0006642968180612163\n",
            "\n",
            "\n",
            "\n",
            "<s> the company </s>\n",
            "probability of:  0.0005298060134788247\n",
            "\n",
            "\n",
            "\n",
            "<s> it said it said </s>\n",
            "probability of:  3.291263149833008e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> the company said it said </s>\n",
            "probability of:  1.586591856256916e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> the us agriculture department said </s>\n",
            "probability of:  8.137147368641228e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> the us agriculture committee </s>\n",
            "probability of:  9.174829959369047e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> it said it said it said </s>\n",
            "probability of:  7.860780254169949e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> it has a share </s>\n",
            "probability of:  6.028753240826936e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> it has a year </s>\n",
            "probability of:  4.092416948332726e-07\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQJ6kNjGDRTi"
      },
      "source": [
        "**Trigram sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFCVaOA9_EMD",
        "outputId": "48460b0b-c3f7-4b38-d45d-a1db8510a9a2"
      },
      "source": [
        "trigram_sentences = []\n",
        "\n",
        "#generate the sentences\n",
        "for i in range(0,10000):\n",
        "    test, probability = ngram_random_sentence(trigram_model_smoothed, [\"<s>\",\"<s>\"],n=2)\n",
        "\n",
        "    trigram_sentences.append((test,probability))\n",
        "\n",
        "\n",
        "trigram_sentences = [(\" \".join(words), prob) for words, prob in trigram_sentences]\n",
        "trigram_sentences_most_probable = heapq.nlargest(num_sentences, set(trigram_sentences), key=operator.itemgetter(1))\n",
        "    \n",
        "print_sentences_joined(trigram_sentences_most_probable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> <s> it said </s>\n",
            "probability of:  0.004496529521320802\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the company said </s>\n",
            "probability of:  0.002043716639246769\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the company </s>\n",
            "probability of:  0.0010243482456935642\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the bank said </s>\n",
            "probability of:  0.00034549887440574493\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the bank of england said </s>\n",
            "probability of:  1.6589274228118742e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> it said the company said </s>\n",
            "probability of:  1.284371903484674e-05\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> it said the company </s>\n",
            "probability of:  6.437507435656652e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> it said the us agriculture department said </s>\n",
            "probability of:  1.3739909232705634e-06\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the company said it will be listed in luxembourg </s>\n",
            "probability of:  5.631226728508644e-07\n",
            "\n",
            "\n",
            "\n",
            "<s> <s> the bank of japan governor haruo maekawa </s>\n",
            "probability of:  1.325993441202779e-07\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRzLvANJZtEm"
      },
      "source": [
        "# **Grading Rubric**\n",
        "This assignment was worth 60 points total. The rubic used for grading this homework is below.\n",
        "\n",
        "**Part 0 - Process the training data (5 points)**\n",
        "\n",
        "**Part 1 - 3 language models (10 points)**\n",
        "- -34 if no probabilities are reported\n",
        "\n",
        "**Part 2 - random sentence generation (15 points)**\n",
        "- -6 if no probabilities are reported\n",
        "\n",
        "**Part 3 - handling unknown tokens and smoothing n-gram models(7 points)**\n",
        "- -3 smoothing is not done\n",
        "- -4 unknown tokens are not handled.\n",
        "\n",
        "**Part 4 - sentence generation (5 points)**\n",
        "\n",
        "**Part 5 - perplexity for 3 language models (15 points)**\n",
        "\n",
        "**Part 6 - summary and reflection (3 points)** \n",
        "\n",
        "\n",
        "**Extra Credit (10 points max)**\n",
        "\n",
        "- +10 generate the 10 most probable sentences (instead of random sentences) for each of the 3 language models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKPfP_WFxG76"
      },
      "source": [
        "# Resources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpy7C-Pmxsed"
      },
      "source": [
        "I used the idea of this function to make my models.  It came from the link included at the beginning of Task 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f01-EpGdxQWm"
      },
      "source": [
        "# MIT License\n",
        "\n",
        "# Copyright (c) 2018 Gaurav Bhatt\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in all\n",
        "# copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "\n",
        "def train_bigram(lst):\n",
        "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "    for sent in lst:\n",
        "        sent = sent.split()\n",
        "        for w1, w2 in bigrams(sent, pad_right=True, pad_left=True):\n",
        "            model[w1][w2] += 1  \n",
        "    total_count = 0      \n",
        "    for w1 in model:\n",
        "        total_count = float(sum(model[w1].values()))\n",
        "        for w2 in model[w1]:\n",
        "            model[w1][w2] /= total_count\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}